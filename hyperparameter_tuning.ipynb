{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31b81816-d33b-485e-bf8c-06869061b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from  IPython import display\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "452c0b4c-a76b-426c-9b81-7a0189a0d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8787e706-49e3-4590-bccf-fbc00b2c75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16635,)\n",
      "(16635, 63)\n",
      "Index(['FGA_HOME', 'FG_PCT_HOME', 'FG3A_HOME', 'FG3_PCT_HOME', 'FTA_HOME',\n",
      "       'FT_PCT_HOME', 'OREB_HOME', 'DREB_HOME', 'AST_HOME', 'STL_HOME',\n",
      "       'BLK_HOME', 'TOV_HOME', 'PF_HOME', 'FGA_AWAY', 'FG_PCT_AWAY',\n",
      "       'FG3A_AWAY', 'FG3_PCT_AWAY', 'FTA_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY',\n",
      "       'DREB_AWAY', 'AST_AWAY', 'STL_AWAY', 'BLK_AWAY', 'TOV_AWAY', 'PF_AWAY',\n",
      "       'PTS_PAINT_HOME', 'PTS_2ND_CHANCE_HOME', 'PTS_FB_HOME',\n",
      "       'TEAM_TURNOVERS_HOME', 'TOTAL_TURNOVERS_HOME', 'TEAM_REBOUNDS_HOME',\n",
      "       'PTS_OFF_TO_HOME', 'PTS_PAINT_AWAY', 'PTS_2ND_CHANCE_AWAY',\n",
      "       'PTS_FB_AWAY', 'TEAM_TURNOVERS_AWAY', 'TOTAL_TURNOVERS_AWAY',\n",
      "       'TEAM_REBOUNDS_AWAY', 'cluster_0h', 'cluster_1h', 'cluster_2h',\n",
      "       'cluster_3h', 'cluster_4h', 'cluster_5h', 'cluster_6h', 'cluster_7h',\n",
      "       'cluster_8h', 'cluster_9h', 'cluster_10h', 'cluster_11h', 'cluster_0a',\n",
      "       'cluster_1a', 'cluster_2a', 'cluster_3a', 'cluster_4a', 'cluster_5a',\n",
      "       'cluster_6a', 'cluster_7a', 'cluster_8a', 'cluster_9a', 'cluster_10a',\n",
      "       'cluster_11a'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/nba_final_data.csv\")\n",
    "data = data.sample(frac=1) # Shuffle data\n",
    "y = data['PLUS_MINUS_HOME']\n",
    "X = data.drop(['GAME_ID', \"TEAM_ID_HOME\", \"TEAM_ID_AWAY\", \"GAME_DATE\", \"SEASON\",\n",
    "               \"PLUS_MINUS_HOME\", \"MIN_HOME\", 'WL_Home_modified'], axis=1)\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "442862cc-debe-4e74-9859-b05efdee750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9e19004-23d6-4572-9955-3be404bf8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: \n",
      " [[84.6   0.46 27.9   0.35 22.5   0.72 11.3  35.4  22.6   6.5   3.7  14.2\n",
      "  19.9  85.8   0.44 22.3   0.35 22.8   0.73  9.6  31.7  19.7   8.4   3.8\n",
      "  11.8  18.3  40.6  16.7  10.4   0.4  14.2   9.5  16.8  39.6  11.7   7.9\n",
      "   0.7  11.8   8.1   0.    0.    1.    0.    1.    4.    0.    0.    0.\n",
      "   3.    0.    1.    2.    0.    1.    0.    0.    1.    0.    0.    0.\n",
      "   5.    1.    0.  ]]\n",
      "\n",
      "Normalized: \n",
      " [[ 0.5   0.13  1.12 -0.25 -0.62 -0.98  0.16  1.36  0.08 -0.98 -1.28 -0.17\n",
      "  -0.36  0.79 -0.28  0.23 -0.1  -0.21 -1.09 -0.84  0.22 -0.55  0.93 -0.86\n",
      "  -1.97 -1.55 -0.43  1.82 -0.81 -0.82 -0.17  0.86  0.39 -0.25 -0.72 -1.68\n",
      "   0.32 -1.97 -0.28 -1.03 -0.01  0.04  0.    1.44  0.99 -1.07  0.   -0.8\n",
      "  -0.75 -0.98  0.77  1.33  0.    0.04  0.   -0.6  -1.09 -1.06  0.   -0.8\n",
      "   0.38  0.69 -0.79]]\n"
     ]
    }
   ],
   "source": [
    "# Define a normalizer for the data\n",
    "normalizer =tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))\n",
    "\n",
    "first = np.array(X_train[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example: \\n', first)\n",
    "  print()\n",
    "  print('Normalized: \\n', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4788f5d5-d0e7-429e-a783-2dbc729b776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an example of a normalizer \n",
    "# fga_model = tf.keras.Sequential([fga_normalizer,\n",
    "#                                 layers.Dense(units=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe63fa4-df82-4f7d-8809-83ba6ae0571e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Hyperband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3a522182-1113-455d-a056-1d6bc4e3c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log dir \n",
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9c58ba70-a019-4e4a-a2fa-f67b74d8e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(normalizer)\n",
    "    # hyperparameter boolean for performing dropout \n",
    "    dropout = hp.Boolean(\"dropout\") \n",
    "    # hyperparameter for percent of units to dropout \n",
    "    if dropout:\n",
    "        drop_percent = hp.Choice(\"drop_percent\", [0.1, 0.25, 0.5])\n",
    "        \n",
    "    kernel_regularizer= regularizers.l2(0.001)\n",
    "    \n",
    "    # hyperparameter for choice of regularization strength\n",
    "    regularization = hp.Choice(\"regularization_strength\", [0.0001, 0.001, 0.01])\n",
    "    \n",
    "    model.add(keras.layers.Flatten(input_shape=(len(X_train.columns),)))\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        hp_units = hp.Int('units', min_value=8, max_value=64, step=4)\n",
    "        model.add(\n",
    "            keras.layers.Dense(units=hp_units,\n",
    "                               activation='elu',\n",
    "                               kernel_regularizer = regularizers.l2(regularization))\n",
    "        )\n",
    "        # Add dropout layer if dropout hyperparameter is True\n",
    "        if dropout:\n",
    "            keras.layers.Dropout(drop_percent)\n",
    "            \n",
    "    model.add(keras.layers.Dense(1)) # output layer\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa2e93-a5ea-45e4-9fba-7a7646e6dcd5",
   "metadata": {},
   "source": [
    "Bayesian optimization for parameter selection, used below, provides better performance than kt.Hyperband()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5117426b-8736-4c88-84c8-cd53c299bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = kt.Hyperband(model_builder,\n",
    "#                      objective='val_mse',\n",
    "#                      max_epochs=5000,\n",
    "#                      factor=3,\n",
    "#                      directory='HP_Tuner_log',\n",
    "#                      project_name='NBAPredict',\n",
    "#                      overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "56c84b41-3447-4883-9ce3-7a427a3e6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c897b980-7912-49fd-b04e-304c9aa28042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1805 Complete [00h 00m 02s]\n",
      "val_mse: 174.9766845703125\n",
      "\n",
      "Best val_mse So Far: 150.0090789794922\n",
      "Total elapsed time: 03h 31m 57s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# tuner.search(X_train, y_train, validation_split=0.2, callbacks=get_callbacks('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a2f35ce4-5cd5-4ccd-bbdf-82984d8dbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# # Get the optimal hyperparameter model\n",
    "# best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# # Build and save it for future use \n",
    "# best_model = tuner.hypermodel.build(best_hps)\n",
    "# best_model.save('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "39f783de-087f-47c9-a1c7-59a7a5d1d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "Num_layers: 2\n",
      "Num_units: 28\n",
      "Dropout: False\n",
      "Dropout rate: 0.1\n",
      "Regularization strength: 0.01\n",
      "Learning Rate: 0.01\n",
      "Epochs: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete.\n",
    "# Num_layers: {best_hps.get('num_layers')}\n",
    "# Num_units: {best_hps.get('units')}\n",
    "# Dropout: {best_hps.get('dropout')}\n",
    "# Dropout rate: {best_hps.get('drop_percent')}\n",
    "# Regularization strength: {best_hps.get(\"regularization_strength\")}\n",
    "# Learning Rate: {best_hps.get('learning_rate')}\n",
    "# Epochs: {best_hps.get('tuner/epochs')}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "842c2ef0-bdd1-49dd-9ce3-bafe20bc5373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/333 [..............................] - ETA: 1:23 - loss: 151.7632 - mse: 151.1052WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0015s). Check your callbacks.\n",
      "314/333 [===========================>..] - ETA: 0s - loss: 164.2338 - mse: 163.4616\n",
      "Epoch: 0, loss:163.4794,  mse:162.7029,  val_loss:155.6715,  val_mse:154.8058,  \n",
      "333/333 [==============================] - 1s 1ms/step - loss: 163.4794 - mse: 162.7029 - val_loss: 155.6715 - val_mse: 154.8058\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 159.3063 - mse: 158.3557 - val_loss: 155.4362 - val_mse: 154.3562\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 157.7397 - mse: 156.4931 - val_loss: 155.8672 - val_mse: 154.5156\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 156.0404 - mse: 154.5315 - val_loss: 154.3244 - val_mse: 152.6747\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 155.2688 - mse: 153.4204 - val_loss: 153.5549 - val_mse: 151.5952\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 153.6831 - mse: 151.5240 - val_loss: 155.2491 - val_mse: 152.9243\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 153.0731 - mse: 150.5119 - val_loss: 157.1419 - val_mse: 154.4513\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 152.1071 - mse: 149.2411 - val_loss: 156.0486 - val_mse: 153.0876\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 150.8858 - mse: 147.7213 - val_loss: 155.8829 - val_mse: 152.5795\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 149.9050 - mse: 146.3966 - val_loss: 157.8809 - val_mse: 154.2052\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 149.1526 - mse: 145.2982 - val_loss: 159.9357 - val_mse: 155.9094\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 148.3972 - mse: 144.2339 - val_loss: 162.7437 - val_mse: 158.4324\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 147.9207 - mse: 143.4236 - val_loss: 160.3680 - val_mse: 155.7587\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 146.4964 - mse: 141.7051 - val_loss: 161.4699 - val_mse: 156.5745\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 146.0011 - mse: 140.9320 - val_loss: 163.6077 - val_mse: 158.4352\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 145.6147 - mse: 140.2674 - val_loss: 165.4178 - val_mse: 159.9216\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 144.5815 - mse: 138.9479 - val_loss: 167.9626 - val_mse: 162.2163\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 143.3544 - mse: 137.4678 - val_loss: 166.9603 - val_mse: 160.9496\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 143.7367 - mse: 137.6014 - val_loss: 169.0957 - val_mse: 162.8641\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 142.4666 - mse: 136.1224 - val_loss: 166.7297 - val_mse: 160.2780\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 142.3294 - mse: 135.7699 - val_loss: 168.4877 - val_mse: 161.8228\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.7122 - mse: 133.9384 - val_loss: 169.8897 - val_mse: 163.0798\n",
      "Epoch 23/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.5463 - mse: 133.5594 - val_loss: 169.4061 - val_mse: 162.3494\n",
      "Epoch 24/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.3127 - mse: 133.1518 - val_loss: 171.5576 - val_mse: 164.2666\n",
      "Epoch 25/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.3575 - mse: 131.9767 - val_loss: 173.5273 - val_mse: 166.0805\n",
      "Epoch 26/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.8764 - mse: 131.3456 - val_loss: 173.6585 - val_mse: 166.0756\n",
      "Epoch 27/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 137.3690 - mse: 129.6750 - val_loss: 180.4453 - val_mse: 172.6731\n",
      "Epoch 28/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 137.9789 - mse: 130.1475 - val_loss: 175.8910 - val_mse: 167.9512\n",
      "Epoch 29/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 136.9754 - mse: 128.9629 - val_loss: 179.6947 - val_mse: 171.6276\n",
      "Epoch 30/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 136.3150 - mse: 128.1277 - val_loss: 178.1142 - val_mse: 169.9207\n",
      "Epoch 31/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 135.7926 - mse: 127.4697 - val_loss: 177.3403 - val_mse: 168.9767\n",
      "Epoch 32/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 136.2609 - mse: 127.7827 - val_loss: 172.8075 - val_mse: 164.3545\n",
      "Epoch 33/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 135.2904 - mse: 126.6957 - val_loss: 180.4108 - val_mse: 171.6788\n",
      "Epoch 34/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 134.9869 - mse: 126.1276 - val_loss: 184.9232 - val_mse: 176.0184\n",
      "Epoch 35/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 135.0664 - mse: 126.0579 - val_loss: 184.9343 - val_mse: 175.8906\n",
      "Epoch 36/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 134.1501 - mse: 125.0577 - val_loss: 184.3556 - val_mse: 175.1958\n",
      "Epoch 37/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 134.3828 - mse: 125.1450 - val_loss: 179.8548 - val_mse: 170.6072\n",
      "Epoch 38/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 133.6084 - mse: 124.3121 - val_loss: 182.1007 - val_mse: 172.7604\n",
      "Epoch 39/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 133.5693 - mse: 124.1166 - val_loss: 182.9944 - val_mse: 173.5660\n",
      "Epoch 40/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 132.7173 - mse: 123.1962 - val_loss: 183.8864 - val_mse: 174.3479\n",
      "Epoch 41/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 132.3793 - mse: 122.7831 - val_loss: 188.7671 - val_mse: 179.1227\n",
      "Epoch 42/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 132.3246 - mse: 122.5865 - val_loss: 184.8945 - val_mse: 175.1421\n",
      "Epoch 43/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 133.2195 - mse: 123.3658 - val_loss: 186.6881 - val_mse: 176.8404\n",
      "Epoch 44/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 132.6238 - mse: 122.7004 - val_loss: 185.5426 - val_mse: 175.6308\n",
      "Epoch 45/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.7634 - mse: 121.7717 - val_loss: 187.2829 - val_mse: 177.3160\n",
      "Epoch 46/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.1476 - mse: 121.0771 - val_loss: 185.1395 - val_mse: 174.9514\n",
      "Epoch 47/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.6503 - mse: 121.3594 - val_loss: 185.9881 - val_mse: 175.7776\n",
      "Epoch 48/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.3881 - mse: 121.0841 - val_loss: 190.9662 - val_mse: 180.6833\n",
      "Epoch 49/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.2525 - mse: 120.9020 - val_loss: 189.7276 - val_mse: 179.3366\n",
      "Epoch 50/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 130.6291 - mse: 120.2134 - val_loss: 191.4205 - val_mse: 180.9732\n",
      "Epoch 51/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 131.4198 - mse: 120.8994 - val_loss: 189.4438 - val_mse: 178.9241\n",
      "Epoch 52/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 130.2173 - mse: 119.6126 - val_loss: 188.7420 - val_mse: 178.1728\n",
      "Epoch 53/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 129.8500 - mse: 119.2114 - val_loss: 191.8164 - val_mse: 181.1437\n",
      "Epoch 54/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 129.8217 - mse: 119.0527 - val_loss: 191.4389 - val_mse: 180.6288\n",
      "Epoch 55/100\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 129.8116 - mse: 118.9469 - val_loss: 193.2931 - val_mse: 182.4033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x228d86ba850>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=get_callbacks('best_model_hyperband'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042a46c-d902-47c9-bc59-b0016c0718a6",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning With Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3c62f1a-a605-421d-92af-104e9da886de",
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_tuner = kt.BayesianOptimization(model_builder,\n",
    "                                  objective='val_mse',\n",
    "                                  max_trials = 100,\n",
    "                                  seed=42,\n",
    "                                  overwrite=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "52560f54-b535-40d9-bbe9-ee1c2485eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 06s]\n",
      "val_mse: 151.0582275390625\n",
      "\n",
      "Best val_mse So Far: 148.90658569335938\n",
      "Total elapsed time: 00h 12m 45s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "BO_tuner.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=get_callbacks('bo_tuner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "693b7e91-bf91-4dbe-9e4d-bd56a3d0fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bo_best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameter model\n",
    "best_hps=BO_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Build and save it for future use \n",
    "best_bo_model = BO_tuner.hypermodel.build(best_hps)\n",
    "best_bo_model.save('bo_best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d70627df-856d-4247-8125-de13a660567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "Num_layers: 2\n",
      "Num_units: 16\n",
      "Dropout: False\n",
      "Dropout rate: 0.5\n",
      "Regularization strength: 0.01\n",
      "Learning Rate: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "Num_layers: {best_hps.get('num_layers')}\n",
    "Num_units: {best_hps.get('units')}\n",
    "Dropout: {best_hps.get('dropout')}\n",
    "Dropout rate: {best_hps.get('drop_percent')}\n",
    "Regularization strength: {best_hps.get(\"regularization_strength\")}\n",
    "Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e495961d-5099-40cb-b522-c54cc7452fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/333 [..............................] - ETA: 1:27 - loss: 111.8931 - mse: 111.4628WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0021s). Check your callbacks.\n",
      "316/333 [===========================>..] - ETA: 0s - loss: 162.8749 - mse: 162.3655\n",
      "Epoch: 0, loss:162.9655,  mse:162.4525,  val_loss:153.0750,  val_mse:152.4841,  \n",
      "333/333 [==============================] - 1s 1ms/step - loss: 162.9655 - mse: 162.4525 - val_loss: 153.0750 - val_mse: 152.4841\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 157.8917 - mse: 157.2089 - val_loss: 154.8928 - val_mse: 154.1415\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 156.6935 - mse: 155.8255 - val_loss: 152.1768 - val_mse: 151.2120\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 155.6066 - mse: 154.5213 - val_loss: 152.1632 - val_mse: 150.9729\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 154.7771 - mse: 153.4874 - val_loss: 152.0578 - val_mse: 150.6575\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 153.6264 - mse: 152.0679 - val_loss: 153.9864 - val_mse: 152.3456\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 152.9561 - mse: 151.1856 - val_loss: 152.7956 - val_mse: 150.9706\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 151.5207 - mse: 149.5587 - val_loss: 154.2355 - val_mse: 152.1616\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 151.5859 - mse: 149.4009 - val_loss: 152.8722 - val_mse: 150.6195\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 150.2091 - mse: 147.8326 - val_loss: 152.7605 - val_mse: 150.2795\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 149.2226 - mse: 146.6366 - val_loss: 155.8726 - val_mse: 153.1859\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 148.6761 - mse: 145.8658 - val_loss: 156.2395 - val_mse: 153.3340\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 148.1946 - mse: 145.1812 - val_loss: 156.0047 - val_mse: 152.9332\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 147.6110 - mse: 144.3971 - val_loss: 156.4254 - val_mse: 153.1414\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 146.7199 - mse: 143.3052 - val_loss: 156.3977 - val_mse: 152.9229\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 145.5645 - mse: 141.9734 - val_loss: 162.6783 - val_mse: 159.0295\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 145.4741 - mse: 141.7352 - val_loss: 159.4254 - val_mse: 155.6257\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 144.7697 - mse: 140.8673 - val_loss: 159.8670 - val_mse: 155.9172\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 144.3493 - mse: 140.3111 - val_loss: 160.2090 - val_mse: 156.1219\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 143.4978 - mse: 139.2983 - val_loss: 160.6977 - val_mse: 156.4551\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 143.8320 - mse: 139.5006 - val_loss: 161.0977 - val_mse: 156.7484\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 142.8254 - mse: 138.3932 - val_loss: 161.4653 - val_mse: 156.9755\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 142.6215 - mse: 138.0643 - val_loss: 162.3258 - val_mse: 157.7003\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 142.4232 - mse: 137.7601 - val_loss: 162.8615 - val_mse: 158.1558\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 141.7233 - mse: 136.9569 - val_loss: 167.1195 - val_mse: 162.2923\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 141.8006 - mse: 136.9130 - val_loss: 164.7519 - val_mse: 159.8511\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 141.8311 - mse: 136.8858 - val_loss: 166.4491 - val_mse: 161.4655\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 141.6184 - mse: 136.5367 - val_loss: 167.7136 - val_mse: 162.6006\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 141.2044 - mse: 136.0487 - val_loss: 166.7568 - val_mse: 161.5492\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.7064 - mse: 135.4495 - val_loss: 168.5452 - val_mse: 163.2738\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.9635 - mse: 135.6326 - val_loss: 166.7739 - val_mse: 161.4064\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.7919 - mse: 135.3712 - val_loss: 165.9080 - val_mse: 160.4823\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 140.3160 - mse: 134.8392 - val_loss: 167.2717 - val_mse: 161.7991\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.9610 - mse: 134.4287 - val_loss: 168.7974 - val_mse: 163.2235\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.9535 - mse: 134.3517 - val_loss: 167.3949 - val_mse: 161.7505\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.2440 - mse: 133.5470 - val_loss: 171.8956 - val_mse: 166.1679\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.5221 - mse: 133.7726 - val_loss: 169.1775 - val_mse: 163.4137\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.9383 - mse: 133.1214 - val_loss: 173.1398 - val_mse: 167.2610\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.9073 - mse: 132.9955 - val_loss: 170.4655 - val_mse: 164.5578\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 138.6207 - mse: 132.6659 - val_loss: 169.7191 - val_mse: 163.7497\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 139.2364 - mse: 133.1962 - val_loss: 167.6386 - val_mse: 161.6080\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.5402 - mse: 132.4692 - val_loss: 172.1959 - val_mse: 166.0973\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.5287 - mse: 132.3868 - val_loss: 171.0838 - val_mse: 164.9030\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.4694 - mse: 132.2957 - val_loss: 172.0310 - val_mse: 165.8172\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 138.3235 - mse: 132.0408 - val_loss: 171.4806 - val_mse: 165.2037\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.3104 - mse: 131.9911 - val_loss: 170.8328 - val_mse: 164.5206\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.7765 - mse: 132.4133 - val_loss: 171.4418 - val_mse: 165.0799\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.5152 - mse: 132.1076 - val_loss: 170.9177 - val_mse: 164.4911\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 137.9391 - mse: 131.4753 - val_loss: 172.3808 - val_mse: 165.9160\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 0s 1ms/step - loss: 138.0703 - mse: 131.5502 - val_loss: 171.7349 - val_mse: 165.2279\n"
     ]
    }
   ],
   "source": [
    "history = best_bo_model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=get_callbacks('best_model_hyperband'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "954fb834-4223-451d-8bc7-3f8a80c60bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_mse = history.history['val_mse']\n",
    "min_mse = min(validation_mse)\n",
    "min_idx = validation_mse.index(min_mse)\n",
    "num_epochs = min_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1d141290-779e-49f8-ad46-ee52b79c2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "  1/416 [..............................] - ETA: 1:41 - loss: 101.7340 - mse: 101.3230WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0017s). Check your callbacks.\n",
      "383/416 [==========================>...] - ETA: 0s - loss: 161.2331 - mse: 160.7069\n",
      "Epoch: 0, loss:161.5488,  mse:161.0164,  \n",
      ".WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 1s 1ms/step - loss: 161.5488 - mse: 161.0164\n",
      "Epoch 2/9\n",
      "415/416 [============================>.] - ETA: 0s - loss: 157.2725 - mse: 156.5309.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 987us/step - loss: 157.3260 - mse: 156.5841\n",
      "Epoch 3/9\n",
      "362/416 [=========================>....] - ETA: 0s - loss: 156.4023 - mse: 155.4083.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 984us/step - loss: 156.2261 - mse: 155.2148\n",
      "Epoch 4/9\n",
      "379/416 [==========================>...] - ETA: 0s - loss: 153.7418 - mse: 152.4269.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 991us/step - loss: 154.6141 - mse: 153.2884\n",
      "Epoch 5/9\n",
      "392/416 [===========================>..] - ETA: 0s - loss: 153.3921 - mse: 151.7580.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 1ms/step - loss: 153.7555 - mse: 152.1134\n",
      "Epoch 6/9\n",
      "374/416 [=========================>....] - ETA: 0s - loss: 152.4332 - mse: 150.5058.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 1ms/step - loss: 152.8134 - mse: 150.8722\n",
      "Epoch 7/9\n",
      "388/416 [==========================>...] - ETA: 0s - loss: 151.6173 - mse: 149.4047.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 1s 1ms/step - loss: 151.2683 - mse: 149.0466\n",
      "Epoch 8/9\n",
      "370/416 [=========================>....] - ETA: 0s - loss: 150.0219 - mse: 147.5034.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 1ms/step - loss: 151.1224 - mse: 148.5929\n",
      "Epoch 9/9\n",
      "393/416 [===========================>..] - ETA: 0s - loss: 150.0835 - mse: 147.3098.WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mse\n",
      "416/416 [==============================] - 0s 984us/step - loss: 150.5116 - mse: 147.7326\n",
      "INFO:tensorflow:Assets written to: final_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_bo_model = BO_tuner.hypermodel.build(best_hps)\n",
    "history=best_bo_model.fit(X_train, y_train, epochs=num_epochs, callbacks=get_callbacks('best_model_hyperband'))\n",
    "best_bo_model.save('final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c8bf0ea4-c2d8-4216-be8d-74f80f3ec5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 853us/step - loss: 150.5481 - mse: 147.6878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 150.54806518554688, 'mse': 147.68775939941406}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = best_bo_model.evaluate(X_test, y_test)\n",
    "dict(zip(best_bo_model.metrics_names, test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
